volumes:
  ollama-data: {}
  openwebui-data: {}
  openwebui-app: {}

networks:
  ollama:
    name: traefik
    external: true

services:
  #ollama service for AMD GPU
  ollama:
    #image: ollama/ollama:latest
    image: ollama/ollama:rocm
    container_name: ollama
    volumes:
      - ollama-data:/root/.ollama
    restart: unless-stopped
    networks:
      - ollama
    ports:
      - 11434:11434
    devices:
      - /dev/dri
      - /dev/kfd
    security_opt:
      - "seccomp:unconfined"
    group_add:
      - video
    environment:
      - HSA_OVERRIDE_GFX_VERSION=9.0.0
      #- 'HCC_AMDGPU_TARGETS=gfx900'
      - HSA_ENABLE_SDMA=0
      - OLLAMA_KEEP_ALIVE=25m

  open-webui:
    image: 'ghcr.io/open-webui/open-webui:main'
    restart: unless-stopped
    container_name: open-webui
    networks:
      - ollama
    # ports:
    #   - 8081:8080
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    volumes:
      - openwebui-data:/data/docs
      - openwebui-app:/app/backend/data
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.ollama.rule=Host(`ollama.local.lan`)"
      - "traefik.http.services.ollama.loadbalancer.server.port=8080"

  #A gateway/proxy for cloud AI services. Little LLM config for Azure openAI inclsuded
  # litellm:
  #   image: ghcr.io/berriai/litellm:main-latest
  #   hostname: litellm
  #   restart: unless-stopped
  #   networks:
  #     - ollama
  #   env_file:
  #     - .env
  #   volumes:
  #     - /etc/localtime:/etc/localtime:ro
  #     - ./litellm_config.yaml:/app/config.yaml
  #   command: [ "--config", "/app/config.yaml", "--port", "4000", "--num_workers", "4" ]


